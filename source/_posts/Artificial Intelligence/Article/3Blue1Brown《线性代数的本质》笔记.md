---
title: 3Blue1Brown《线性代数的本质》笔记
author: Liu Yibang
date: 2024/08/24
categories: 
    - Artificial Intelligence
    - Article
mathjax: true
---

> [官方中文](https://www.bilibili.com/video/BV1ys411472E/)

> 尽管一批教授和教科书编者用关于矩阵的荒唐至极的计算内容
> 
> 掩盖了线性代数的简明性，但是鲜有与之相较更为初等的理论。
>
> —— 让·迪厄多内

国内的线性代数教材往往过分专注于其**数值计算**的部分而对其几何含义缺少解释，而线性代数有许多被教材所忽视的、可视化的直观理解，当真正理解了几何直观与数值计算的关系时，线性代数的细节和其在各种领域的应用就会显得合情合理

更重要的是，关于数值计算的部分，现代已经拥有了计算机来帮我们出来这部分问题，在实践中，我们更应当去关注概念层面的东西

基于相同的道理，这篇文章仅用于个人的学习笔记，在涉及到相关概念的理解上，个人强烈建议去观看作者的原视频

## 向量

三种看待向量的视角：
- 物理专业：空间中的箭头，由方向和长度所定义，一旦向量被定义就可以在空间中任意移动而不被改变
- 计算机专业：有序数列，此时向量只是一个“列表”的花哨说法
- 数学专业：概况以上两种观点，向量可以是任何东西——只要能保证向量相加和向量与数字相乘是有意义的即可（最抽象的概念）

**向量加法和向量数乘贯穿线性代数的始终**

和物理专业的看法有一定出入的是，线性代数中的向量往往**以坐标原点起始**
- 当向量以坐标原点起始时，就可以通过一个有序列表来表示向量的坐标，对应了计算机专业中的看法
- 以二维坐标系为例，向量的坐标由一对数构成，描述了如何从向量的起点（也就是原点）出发达到向量的终点，从而保证了**向量**和**坐标**是一一对应的关系
- 和点坐标不同的是，向量坐标往往竖着写，以和点坐标区分开，例如：

$$
\begin{bmatrix}
    -2 \\\\
    3
\end{bmatrix}
$$

### 向量相加

![](/images/posts/linear-vector-1.png)

以 $\vec{v}+\vec{w}$ 为例，即将 $\vec{w}$ 平移，使其起点对准 $\vec{v}$ 的终点，最终画一条从 $\vec{v}$ 起点指向 $\vec{w}$ 终点的向量
- 将向量看作空间上的移动，向量的加法则代表了移动的累加
- 先沿着 $\vec{v}$ 移动，再从终点沿着将 $\vec{w}$ 移动，在效果上等价于沿着 $\vec{v}+\vec{w}$ 移动

向量加法的原理可以同样延伸到数轴上——以 $2-5$ 为例，相当于从一位数轴的原点出发，向右移动 2，再向左移动 5，本质上与直接向左移动 -3 等效

在一维数轴的基础上延伸，二维向量的加法仍然是坐标的累加：

$$
\begin{bmatrix}
    1\\\\
    2
\end{bmatrix}+
\begin{bmatrix}
    3\\\\
    -1
\end{bmatrix}=
\begin{bmatrix}
    4\\\\
    1
\end{bmatrix}
$$

即先沿着 x 坐标轴移动 (1+3) 步，再沿着y坐标轴移动 (2-1) 步，从单个坐标轴的角度上看，这与前面提到的一维数轴的加法是一样的

### 向量数乘（缩放）

![](/images/posts/linear-vector-2.png)

向量数乘即将原向量延长\缩短为原本的n倍，当n为负数时，则说明运算结果的向量方向与原向量相反

$$
2\cdot
\begin{bmatrix}
    3\\\\
    1
\end{bmatrix}=
\begin{bmatrix}
    6\\\\
    2
\end{bmatrix}
$$

这类运算被称为向量的**缩放**（Scaling），而其中的$n$被称作**标量**（Scalar）
- 在线性代数中，单独数字唯一的用途基本上就是用于向量的缩放，因此通常来说，数字和标量这两个词可以互相替换

### 向量的线性组合

这里有另一种角度来看待向量的坐标

在二维坐标系中有两个特殊含义的向量
- x轴上的单位向量 $\hat{i}$，也被叫做**i帽**（i-hat）
- y轴上的单位向量 $\hat{j}$，也被叫做**j帽**（j-hat）
- 这些单位向量被叫做**坐标系的基向量**

于是，我们可以将向量的坐标看作**一组标量**，分别描述了该向量是**i帽和j帽分别经过何种缩放后，再进行相加得到的**

![](/images/posts/linear-vector-3.png)

因此，当我们用坐标描述一个向量，它同样依赖于我们**正在使用的基向量**，不同的基向量会导致不同的结果（例如，平面锐角坐标系）

我们可以发现，此时该向量是由两个基向量经过缩放和相加得到的。由此，我们可以引出：两个数乘向量的和被称作这两个向量的**线性组合**，即：

$$
a\vec{v} + b\vec{w}
$$

而在大部分情况下对于一对初始向量的线性组合，如果我们让两个标量都自由变化，我们可以得到一个平面中的任何一个向量
- 例外是当两个向量平行时，此时得到的向量方向被严格限制在了平行的直线上

其中，所有可以由  $a\vec{v} + b\vec{w}$ 得到的向量的集合，被称为**向量 $\vec{v}$ 和 $\vec{w}$ 张成的空间（span）**

因此，我们也可以说：
- 对于大部分二维向量对，它们张成的空间是所有二维向量的集合
- 对于共线的二维向量对，它们张成的空间就是一条直线上的向量的集合

我们扩展到三维空间，不考虑特殊情况，三维空间中的两个向量张成的空间仍然是这两个向量构成的平面，但当加上第三个向量时，我们往往能得到所有的三维向量，这三个向量所有可能的线性组合就会构成整个三维空间
- 同样地，例外是当第三个向量仍落在前两个向量的平面上时，此时得到的向量方向被严格限制在了平面上
- 当我们修改第三个向量的标量时，实际上是在推动前两个向量张成的平面沿着第三个向量的方向移动，从而扫过整个三维空间

在二维空间和三维空间中，都有一种例外情况：
- 我们在向量的线性组合 $a\vec{v} + b\vec{w}$ 中添加了一个向量 $\vec{u}$，但是并没有扩展这个线性组合张成的空间，这个时候这些向量被称作**线性相关的**（Linearly Dependent）
    - 在这种情况下，我们有 $\vec{u} = a\vec{v} + b\vec{w}$，其中a和b取某个值时该式子可以成立
- 反之，如果一组向量中每一组向量都为这个线性组合扩展了新的维度，那么我们就称这一组向量是**线性无关的**（Linearly Independent）
    - 在这种情况下，我们有 $\vec{u} \neq a\vec{v} + b\vec{w}$，其中a和b可以取所有值

基于以上概念，我们可以引出**空间的基**的严格定义：张成该空间的一个线性无关向量的集合

## 矩阵与线性变换

### 线性变换

线性变换（Linear Transformation）：接受一个向量并输出一个向量的变换
- 线性（Linear）指空间中的所有直线在变换后仍然是直线，且原点的位置没有发生改变
- 变换（Transformation）本质上是函数（function）的花哨说法，但与函数不同的是，**变换**一词在刻意地暗示你可以**用可视化的运动来思考这个过程**

![](/images/posts/linear-matrix-1.png)

这张图看似直线没有被弯曲，但是在加上对角线后就会发现对角线变得弯曲了，因此仍然不属于线性变换

![](/images/posts/linear-matrix-2.png)

总的来说，线性变换要保证**网格线平行且等距分布**，在这个大前提下，我们有一个重要的推论
- 变换前的向量 $\vec{v}$ 是**i帽与j帽**的线性组合 $a\hat{i_1} + b\hat{j_1}$，那么变换后的 $\vec{v}$ 仍是**变换后的i帽与j帽**的相同线性组合 $a\hat{i_2} + b\hat{j_2}$
- 换句话说，若在变换前有 $\vec{v} = -1 \hat{i} + 2 \hat{j}$，那么在变换后等式仍然成立，因此只要找到变换后的基向量代入式子，就能得到变换后的向量 $\vec{v}$

举例来说，对于向量

$$
\vec{v} = 
\begin{bmatrix}
    -1\\\\
    2
\end{bmatrix} = -1 \hat{i} + 2 \hat{j} = -1 
\begin{bmatrix}
    1\\\\
    0
\end{bmatrix} + 2 
\begin{bmatrix}
    0\\\\
    1
\end{bmatrix}
$$

在变换后，基向量坐标发生了改变，有

$$
\vec{v} = -1 \hat{i} + 2 \hat{j} = -1 
\begin{bmatrix}
    1\\\\
    -2
\end{bmatrix} + 2 
\begin{bmatrix}
    3\\\\
    0
\end{bmatrix} =
\begin{bmatrix}
    5\\\\
    2
\end{bmatrix}
$$

因此，只要我们知道了变换后的基向量，我们就能推出任意一个向量在变换后的位置。基于这点，我们可以说，一个二维线性变换仅有四个数字决定：变换后的i帽坐标和变换后的j帽坐标。

### 矩阵

于是，我们将这四个数字包装在一个**矩阵**（Matrix）中，用于定义一个线性变换的函数
- 也就是说，矩阵本质上是**对空间操纵的描述**
- 同样地，你也可以通过一个矩阵来想象线性变换的过程，只要分别移动i帽和j帽，然后另空间的其他部分随着基向量一起移动即可
- 例如：将坐标系逆时针旋转90度的矩阵就是：

$$
\begin{bmatrix}
    0 & -1\\\\
    1 & 0
\end{bmatrix}
$$

- 如果两个基向量是线性相关的，就意味着整个二维空间被挤压到它们所在的直线上

$$
\begin{bmatrix}
    2 & -2\\\\
    1 & -1
\end{bmatrix}
$$

同样以上例为例，将变换写成矩阵运算的形式：

$$
\vec{v} = 
\begin{bmatrix}
    1 & 3\\\\
    -2 & 0
\end{bmatrix}
\begin{bmatrix}
    -1\\\\
    2
\end{bmatrix} = -1
\begin{bmatrix}
    1\\\\
    -2
\end{bmatrix} + 2
\begin{bmatrix}
    3\\\\
    0
\end{bmatrix} =
\begin{bmatrix}
    5\\\\
    2
\end{bmatrix}
$$

在这种理解方式下，矩阵的**第一列**代表**变换后的第一个基向量**，**第二列**代表**变换后的第二个基向量**，更加通用的公式是：

$$
\vec{v} = 
\begin{bmatrix}
    a & b\\\\
    c & d
\end{bmatrix}
\begin{bmatrix}
    x\\\\
    y
\end{bmatrix} = x
\begin{bmatrix}
    a\\\\
    c
\end{bmatrix} + y
\begin{bmatrix}
    b\\\\
    d
\end{bmatrix} =
\begin{bmatrix}
    ax+by\\\\
    cx+dy
\end{bmatrix}
$$

## 矩阵乘法与线性变换复合

### 复合变换

我们已经知道了，矩阵是**对空间线性变换的描述函数**。和向量类似，我们也可以对多个线性变换进行累加操作（即先进行线性变换A，再在此基础上进行线性变换B），我们称之为**两个线性变换的复合变换**

举例来说，旋转（Rotation）变换的矩阵是：

$$
\begin{bmatrix}
    0 & -1\\\\
    1 & 0
\end{bmatrix}
$$

剪切（Shear）变换的矩阵是：

$$
\begin{bmatrix}
    1 & 1\\\\
    0 & 1
\end{bmatrix}
$$

通过对旋转后的基向量进行剪切变换，我们可以得到这两个变换的复合变换的矩阵，该矩阵描述了先进行旋转再进行变换后的总效应：

$$
\begin{bmatrix}
    1 & -1\\\\
    1 & 0
\end{bmatrix}
$$

如果我们想对一个向量 $\vec{v}$ 先应用旋转，再应用剪切的话，那么计算公式应当是：

$$
\begin{bmatrix}
    1 & 1\\\\
    0 & 1
\end{bmatrix}
\left(
    \begin{bmatrix}
        0 & -1\\\\
        1 & 0
    \end{bmatrix}
    \begin{bmatrix}
        x\\\\
        y
    \end{bmatrix}
\right) =
\begin{bmatrix}
    1 & -1\\\\
    1 & 0
\end{bmatrix}
\begin{bmatrix}
    x\\\\
    y
\end{bmatrix}
$$

值得一提的是，线性变换的公式是从右往左读的，按照这个公式，应当先应用右侧的旋转矩阵，再应用左侧的剪切矩阵。
- 这个习惯是基于函数写法的延续，举例来说，$f(x)$ 是对 $x$ 应用的函数，这里的 $f$ 也是写在左侧。如果我们把剪切矩阵记作函数 $g$，把旋转矩阵记作函数 $f$，把向量 $\vec{v}$ 记作 $x$ 的话，那么按照函数的写法就应当是 $g(f(x))$，可以看到这个顺序和上面的式子是相同的顺序

### 矩阵乘法

于是，我们称这个复合变换的矩阵为**旋转矩阵和剪切矩阵的积**
- 和向量不同的是，这里是积（乘法）而不是和（加法）

$$
\begin{bmatrix}
    1 & 1\\\\
    0 & 1
\end{bmatrix}
\begin{bmatrix}
    0 & -1\\\\
    1 & 0
\end{bmatrix} =
\begin{bmatrix}
    1 & -1\\\\
    1 & 0
\end{bmatrix}
$$

通过对线性变换过程中的基向量进行跟踪，我们可以很轻松地写出矩阵乘法的公式，对于：

$$
\begin{bmatrix}
    a & b\\\\
    c & d
\end{bmatrix}
\begin{bmatrix}
    e & f\\\\
    g & h
\end{bmatrix}
$$

复合函数的第一列列向量：

$$
\begin{bmatrix}
    a & b\\\\
    c & d
\end{bmatrix}
\begin{bmatrix}
    e\\\\
    g
\end{bmatrix} = e
\begin{bmatrix}
    a\\\\
    c
\end{bmatrix} + g
\begin{bmatrix}
    b\\\\
    d
\end{bmatrix} =
\begin{bmatrix}
    ae+bg\\\\
    ce+dg
\end{bmatrix}
$$

复合函数的第二列列向量：

$$
\begin{bmatrix}
    a & b\\\\
    c & d
\end{bmatrix}
\begin{bmatrix}
    f\\\\
    h
\end{bmatrix} = f
\begin{bmatrix}
    a\\\\
    c
\end{bmatrix} + h
\begin{bmatrix}
    b\\\\
    d
\end{bmatrix} =
\begin{bmatrix}
    af+bh\\\\
    cf+dh
\end{bmatrix}
$$

于是我们得出了复合变换的计算公式：

$$
\begin{bmatrix}
    a & b\\\\
    c & d
\end{bmatrix}
\begin{bmatrix}
    e & f\\\\
    g & h
\end{bmatrix} =
\begin{bmatrix}
    ae+bg & af+bh\\\\
    ce+dg & cf+dh
\end{bmatrix}
$$

- 矩阵乘法不满足交换律，即 $AB \neq BA$
- 矩阵乘法满足结合律，即 $(AB)C = A(BC)$
    - 如果你用公式推导的思路去做这个证明，会发现非常痛苦且没有帮助
    - 只从线性变换的角度上思考，两个公式本质上都描述了**先进行C变换，再进行B变换，最后进行A变换**，因此没有区别

## 行列式

在网格线平行且等距分布（即线性变换）的情况下，空间中任意一个区域的面积在变换后的缩放倍数都应当是一致的

举例来说，对于矩阵：

$$
\begin{bmatrix}
    3 & 2\\\\
    0 & 2
\end{bmatrix}
$$

对于空间中的任意一个区域 $A$，在经过该线性变换后体积变为原来的6倍，即 $6\cdot A$，那么我们称 $6$ 为该线性变换的**行列式**（determinant），即：

$$
det
\left(
    \begin{bmatrix}
        3 & 2\\\\
        0 & 2
    \end{bmatrix}
\right) = 6
$$

也就是说，**行列式**用于描述线性变换改变面积的比例
- 当一个线性变换的行列式为 0 时，说明该线性变换将空间压缩到了一个更小的维度上，即变换后的基向量是线性相关的：

$$
det
\left(
    \begin{bmatrix}
        4 & 2\\\\
        2 & 1
    \end{bmatrix}
\right)
=0
$$

- 当一个线性变换的行列式为负数时，说明该变换本质上将整个二维平面进行了一次翻转，我们也称这样的变换改变了空间的**定向**

推广到三维空间，行列式就是对体积变换比例的描述
- 当在三维空间考虑空间的定向时，可以用右手定则：右手食指是 $\hat{i}$ 的方向，中指是 $\hat{j}$ 的方向，此时大拇指应当自然地指向 $\hat{k}$ 的方向。当变换后如果仍能用右手这样指向，则行列式为正，如果只能用左手来指向，则行列式为负。

### 行列式的计算

先给出一个结论，对于一个二维的矩阵，行列式的计算公式是：

$$
det
\left(
    \begin{bmatrix}
        a & b\\\\
        c & d
    \end{bmatrix}
\right) = ad - bc
$$

如何理解这个公式？假设 b 和 c 均为0，那么这个矩阵实际上是对 $\hat{i}$ 和 $\hat{j}$ 的正向缩放，此时 $\hat{i}$ 沿 x 轴被缩放了 a 倍，而 $\hat{j}$ 沿 y 轴缩放了 d 倍，面积的变换倍率就是 ad 即

$$
det
\left(
    \begin{bmatrix}
        a & 0\\\\
        0 & d
    \end{bmatrix}
\right) = ad
$$

粗略来说，以 $\hat{i}$ 和 $\hat{j}$ 围成的平行四边形为例，即 a 和 d 实际上是代表了这个平行四边形，将**从原点出发的对角线**进行拉伸后的面积倍率；而 b 和 c 则是这个平行四边形，将**对角的对角线**进行拉伸后的面积倍率

从一个更详细的几何角度来说，行列式的推导过程如下：

$$
det
\left(
    \begin{bmatrix}
        a & b\\\\
        c & d
    \end{bmatrix}
\right) = (a+b)(c+d) - ac - bd - 2bc = ad - bc
$$

![Alt text](/images/posts/linear-determinant-1.png)

当一个由 $\hat{i}$ 和 $\hat{j}$ 围成的正方形（面积为1）被变换成如图所示的平行四边形时，可以结合上图看出平行四边形的面积计算公式

对于三维空间的行列式计算，遵循如下公式：

$$
det
\left(
    \begin{bmatrix}
        a & b & c\\\\
        d & e & f\\\\
        g & h & i
    \end{bmatrix}
\right) = a \cdot det
\left(
    \begin{bmatrix}
        e & f\\\\
        h & i
    \end{bmatrix}
\right) - b \cdot det
\left(
    \begin{bmatrix}
        d & f\\\\
        g & i
    \end{bmatrix}
\right) + c \cdot det
\left(
    \begin{bmatrix}
        d & e\\\\
        g & h
    \end{bmatrix}
\right)
$$

然而，三维以及更高维的行列式计算公式的推导过程难以描述，且对于线性代数的理解帮助不大，这里不再赘述
- 如果想要证明，可以自己试一下三维空间中平行六面体体积的推导过程
- 或者，去参考 Sal Khan 的推导过程

### 复合变换的行列式

对于两个矩阵相乘得到的复合矩阵，其行列式也等于两个矩阵的行列式相乘，即：

$$
det(M_1 M_2) = det(M_1)det(M_2)
$$

这个公式从数学角度上推导相当困难，但是类似矩阵乘法的结合律，我们同样可以从几何角度去思考这个问题：当进行了线性变换 $M_2$ 后，区域的面积变为了原来的 $det(M_2)$ 倍，再进行了线性变换 $M_1$ 后，区域的面积在此基础上又变为了刚才的 $det(M_1)$ 倍

## 逆矩阵、列空间与零空间

### 线性方程组

形如

$$
\begin{cases}
    2x+5y+3z=-3\\\\
    4x+0y+8z=0\\\\
    1x+3y+0z=2
\end{cases}
$$

的方程组叫做**线性方程组**

不难注意到，你可以将上述方程组写成**矩阵和向量乘法**的形式：

$$
\begin{bmatrix}
    2 & 5 & 3\\\\
    4 & 0 & 8\\\\
    1 & 3 & 0
\end{bmatrix}
\begin{bmatrix}
    x\\\\
    y\\\\
    z
\end{bmatrix} = 
\begin{bmatrix}
    -3\\\\
    0\\\\
    2
\end{bmatrix}
$$

将该式子展开后得到的方程组和上面的方程组完全相同

我们称该矩阵为系数矩阵 $A$，未知量向量为 $\vec{x}$，常数向量为 $\vec{v}$，于是我们可以将该方程组写作如下形式：

$$
A\vec{x}=\vec{v}
$$

于是，我们求解该方程组的几何意义变为了：给定一个线性变换 $A$ 和一个向量 $\vec{v}$，要找到一个向量 $\vec{x}$，使之经过该线性变换后与向量 $\vec{v}$ 重合

在 $A$ 的行列式不为 0 的情况下，我们可以找到一个唯一的向量 $\vec{x}$ 在经过变换后与 $\vec{v}$ 重合 —— 只需要对向量 $\vec{v}$ 进行该线性变换的逆向变换即可

### 逆矩阵

这个**线性变换 $A$ 的逆向变换**我们记作 $A$ 的逆矩阵 —— $A^{-1}$

在应用变换 $A$ 再应用变换 $A^{-1}$ 后会回到初始状态，也就是对于矩阵乘法 $A^{-1}A$ 会得到一个**什么也不做**的复合变换，我们称之为**恒等变换**

$$
A^{-1}A = 
\begin{bmatrix}
    1 & 0\\\\
    0 & 1
\end{bmatrix}
$$

要得到矩阵 $A$ 的逆矩阵，我们可以通过计算机来计算，这样我们就可以通过在上述方程的两边同时乘以 $A^{-1}$ 来得到答案：

$$
A\vec{x}=\vec{v}\\\\
A^{-1}A\vec{x}=A^{-1}\vec{v}\\\\
\vec{x}=A^{-1}\vec{v}
$$

但是当 $A$ 的行列式为0的情况下，以二维矩阵（二元方程组）为例，空间被压缩为一条直线，我们无法将一条直线**逆向**回一个平面，即**行列式为0的矩阵是不可逆的**，因此该方程组无法求解
- 无法求解并不代表无解，而是该方程组存在多个可行解，无法求出唯一解
- 当一个线性变换降维，对于一个输出向量 $\vec{v}$，必然存在多个可能的输入向量 $\vec{x}$；换句话说，当我们对 $\vec{v}$ 应用线性变换的逆矩阵时，必然有多个可能的输出 $\vec{x}$，而这一点破坏了函数**一个输入只能对应一个输出**的基本性质

### 秩和列空间

我们可以将某个线性变换后空间的维数称作**秩**（rank）
- 若一个线性变换在执行后空间变为了二维（不论空间之前是几维的），那么我们就称该线性变换的秩为2

对于一个线性变换，可能得到的所有输出向量 $A\vec{v}$ 的集合，我们称之为**线性变换 $A$ 的列空间**（Column Space）
- 你可以这么理解：矩阵的每一列代表了变换后的基向量，因此列空间就是**变换后的基向量所张成的空间**

因此，对于**秩**的更确切的定义是：某个线性变换的列空间的维数

线性变换无法扩展维数，因此秩不会超过矩阵的列数。
- 当矩阵的秩与列数相等时，我们称该矩阵为**满秩矩阵**（Full Rank）

对于满秩矩阵而言，由于线性变换必须保证原点位置不变，因此列空间中一定包含零向量，反过来说，零向量也是唯一变换后会落在原点的向量 —— 这也是为什么齐次线性方程组 $A\vec{x} = 0$ 一定有零解的原因，因为有且仅有零向量在经过任何变换后仍然是零向量

当矩阵的秩小于列数时，不仅有零向量在变换后会得到零向量，还有一系列其他方向上的向量被压缩到原点，此时对于齐次线性方程组 $A\vec{x} = 0$，有一系列可能的向量解，我们称之为该矩阵的**零空间**或**核**

## 点积和叉积

### 点积

对于两个相同维度向量的点积，只需要将对应的坐标两两相乘，最后求和即可，即：

$$
\begin{bmatrix}
    a\\\\
    b\\\\
    c
\end{bmatrix}
\cdot
\begin{bmatrix}
    d\\\\
    e\\\\
    f
\end{bmatrix} = ad + bc + cf
$$

对于这个式子的几何含义：以 $\vec{v} \cdot \vec{w}$ 为例，即 $\vec{w}$ 在 $\vec{v}$ 上的投影长度，乘以向量 $\vec{v}$ 的长度
- 当然，点积是满足交换律的，因此该几何含义中将 $\vec{v}$ 和 $\vec{w}$ 的顺序调换也是可以的
- 其中点积为负数表示即 $\vec{w}$ 在 $\vec{v}$ 上的投影与 $\vec{v}$ 的方向相反

![](/images/posts/linear-dotproduct-1.png)

但是为什么点积的几何含义会和向量的投影联系起来？这里需要引入一个叫做**对偶性**的概念

### 点积的对偶性

> 这部分比较难以理解，可能要对原视频多看几遍
> 
> 不过我个人认为，视频中对于将 $\hat{u}$ 推广到非单位向量的情况解释得并不够好，可以结合我的笔记来更好地理解这部分内容

对偶性指**两种数学事物中自然而又出乎意料的对应关系**，接下来我们要证明**一个多维空间到一个一维空间的线性变换的对偶是多维空间中的某个特定向量**

对于会降维的线性变换，要求原本等距分布的点在变换后仍然等距分布的，在这个基础上，我们去考虑如何描述一个降维的线性变换：

我们已经知道了，矩阵描述的是线性变换之后，基向量的坐标。在变换之前，我们在二维空间中拥有两个基向量：$\hat{i}$ 和 $\hat{j}$，但是由于变换之后空间降到了一维，因此变换后的基向量坐标也是一维的，即每个基向量各落在了一维数轴的一个数上，此时该变换的矩阵可能写作下列形式：

$$
\begin{bmatrix}
    1 & -2
\end{bmatrix}
$$

即一个非方矩阵，此时若我们想对一个向量 $\vec{v}$ 应用该线性变换，那么应该写作下列形式：

$$
\begin{bmatrix}
    1 & -2
\end{bmatrix}
\begin{bmatrix}
    4\\\\
    3
\end{bmatrix} = 4
\begin{bmatrix}
    1 
\end{bmatrix} + 3
\begin{bmatrix}
    -2
\end{bmatrix} = 
\begin{bmatrix}
    -2
\end{bmatrix} 
$$

不难注意到，若将该矩阵看作一个**颠倒的向量**，则此时计算方式等同于点积的计算方式

假设存在这样一个变换，将二维空间中的点投影到一条过原点的直线上，我们定义该直线上的基向量为 $\hat{u}$。显然地，这是一个接受二维向量并输出一维向量（数）的降维变换

![](/images/posts/linear-dotproduct-2.png)

要注意的是，图中的点表示二维向量的终点坐标，而不是点，你可以认为每个点都是一个**从原点出发指向该点的向量**

理所当然的，这个变换的矩阵应该形如

$$
\begin{bmatrix}
    a & b
\end{bmatrix}
$$

其中 $a$ 和 $b$ 分别代表了 $\hat{i}$ 和 $\hat{j}$ 投影到该直线上的坐标

![](/images/posts/linear-dotproduct-3.png)

首先考虑 $a$，即 $\hat{i}$ 的部分，由于 $\hat{i}$ 和 $\hat{u}$均为单位向量，他们之间存在一条对称轴将其分开，使之完全对称 —— 这导致了 $\hat{i}$ 在 $\hat{u}$ 上的投影长度等于  $\hat{u}$ 在 $\hat{i}$ 上的投影长度，而后者恰好是 $\hat{u}$ 的 x 轴坐标，即 $u_x$

![](/images/posts/linear-dotproduct-4.png)

将其推广到 $b$，即 $\hat{j}$ 的部分，不难得出 $\hat{j}$ 在 $\hat{u}$ 上的投影长度为 $u_y$

因此该矩阵为：

$$
\begin{bmatrix}
    u_x & u_y
\end{bmatrix}
$$

此时，我们随即给出一个二维空间的向量 $\vec{v}$ ，要求该向量在该直线上的投影，即：

$$
\begin{bmatrix}
    u_x & u_y
\end{bmatrix}
\begin{bmatrix}
    a\\\\
    b
\end{bmatrix} = 
\begin{bmatrix}
    a \cdot u_x + b \cdot u_y
\end{bmatrix} 
$$

要注意的是，以上情况为 $\hat{u}$ 的长度等同于变换前基向量长度（也就是1）的情况，只有最开始 $\hat{u}$ 定义为单位向量的时候， $\hat{u}$ 和 $\hat{i}$、$\hat{j}$ 的对称性才是成立的，此时该线性变换的含义是：求向量到该直线的投影长度

当 $\hat{u}$ 的长度为 $n$ 时，该对称性被破坏，实际上 $\hat{u}$ 在 x 轴上的投影长度是 $\hat{i}$ 在 $\hat{u}$ 上投影长度的 n 倍，要满足对称性，**该线性变换的定义必须发生改变**，我们必须将结果（原先是 $\hat{i}$ 在 $\hat{u}$ 上投影长度）也变为原先的 n 倍，即从原先的**求向量到直线的投影长度**变为**求向量到直线的投影长度和 n 的积**

此时这个线性变换的性质发生了改变，它不再代表**二维空间上的向量在该直线上的投影长度**，而是**二维空间上的向量在该直线上的投影长度乘以 $\hat{u}$ 的长度**，也就是说，对于公式

$$
A\vec{v} = 
\begin{bmatrix}
    u_x & u_y
\end{bmatrix}
\begin{bmatrix}
    a\\\\
    b
\end{bmatrix} = 
\begin{bmatrix}
    a \cdot u_x + b \cdot u_y
\end{bmatrix} 
$$

它代表的含义是向量 $\vec{v}$ 在向量 $\vec{u}$ 上的投影长度，乘以 $\hat{u}$ 的长度，这点**与最开始点积的定义符合**

这一点给我们的启发是：当我们看到一个从高维到一维的线性变换时，必然存在一个向量 $\vec{u}$ 与该变换相关联，对向量 $\vec{v}$ 进行该线性变换等价于对 $\vec{v}$ 和 $\vec{u}$ 做点积 

### 叉积

对于叉积的两个较为重要的属性，对于 $\vec{v} \times \vec{w}$ 有：
- 其**数值**等于两个向量围成的平行四边形的面积
- 其**符号**当 $\vec{v}$ 在 $\vec{w}$ 右侧时数值为正，反之则为负
- 要注意这里只是叉积的两个属性如何求得，并非叉积的定义
- 和点积不同的是，叉积的数值是不满足交换律的，有 $\vec{v} \times \vec{w} = -\vec{v} \times \vec{w}$

![](/images/posts/linear-crossproduct-1.png)

不难注意到，叉积的计算方式和**行列式**的计算方式类似，实际上，我们有如下公式：

$$
\begin{bmatrix}
    a\\\\
    b
\end{bmatrix}
\times
\begin{bmatrix}
    c\\\\
    d
\end{bmatrix} = det
\left(
    \begin{bmatrix}
        a & c\\\\
        b & d
    \end{bmatrix}
\right)
$$

写作如下形式也是可以的，因为矩阵的转置并不会改变行列式的值：


$$
\begin{bmatrix}
    a\\\\
    b
\end{bmatrix}
\times
\begin{bmatrix}
    c\\\\
    d
\end{bmatrix} = det
\left(
    \begin{bmatrix}
        a & b\\\\
        c & d
    \end{bmatrix}
\right)
$$

你可以认为 $\vec{v}$ 和 $\vec{w}$ 是 $\hat{i}$ 和 $\hat{j}$ 经过某线性变换之后得到的基向量，那么此时，如果我们专注于 $\hat{i}$ 和 $\hat{j}$ 围成的平行四边形的面积 —— 变换前为1，而变换后为 $\vec{v} \times \vec{w}$，此时该线性变换的面积缩放倍率（即行列式）恰好为 $\vec{v} \times \vec{w}$，其正负号与行列式一章中行列式的正负号含义一致
- 当我们说 $\vec{v} \times \vec{w}$ 时，$\vec{v}$ 对应变换前的 $\hat{i}$，而 $\vec{w}$ 对应变换前的 $\hat{j}$，考虑到初始状态下 $\hat{i}$（x轴向右）位于 $\hat{j}$（y轴向上）的右侧，因此当且仅当 $\vec{v}$ 在 $\vec{w}$ 右侧时，空间的定向没有发生改变，叉积（即该变换的行列式）为正

要注意，这里我们仍然没有在探讨叉积的真实定义，上面的 $\vec{v} \times \vec{w}$ 都是对其数值和符号的探讨

接下来我们回归到**叉积的确切定义**上：**叉积是通过两个三维向量生成一个新三维向量的过程**
- 生成向量的长度等于上文中提到的 —— 这两个向量围成的平行四边形的面积大小
- 生成向量的方向垂直于该平行四边形，并且对于 $\vec{v} \times \vec{w}$ 遵从以下规则：
    - 右手法则：食指指向 $\vec{v}$ 的方向，中指指向 $\vec{w}$ 的方向，竖起大拇指，此时大拇指的方向就是叉积的方向

对于更一般的情况，在给定两个输入向量的坐标的情况下，输出向量的坐标遵循如下公式：

$$
\begin{bmatrix}
    v_1\\\\
    v_2\\\\
    v_3
\end{bmatrix}\times
\begin{bmatrix}
    w_1\\\\
    w_2\\\\
    w_3
\end{bmatrix} = 
\begin{bmatrix}
    v_2w_3-w_2v_3\\\\
    v_3w_1-w_3v_1\\\\
    v_1w_2-w_1v_2
\end{bmatrix}
$$

该公式可以写作一个更易于记忆的行列式写法：

$$
\begin{bmatrix}
    v_1\\\\
    v_2\\\\
    v_3
\end{bmatrix}\times
\begin{bmatrix}
    w_1\\\\
    w_2\\\\
    w_3
\end{bmatrix} = det
\left(
\begin{bmatrix}
    \hat{i} & v_1 & w_1\\\\
    \hat{j} & v_2 & w_2\\\\
    \hat{k} & v_3 & w_3
\end{bmatrix}
\right)
$$

同样地，写作如下形式也是可以的，因为矩阵的转置并不会改变行列式的值：


$$
\begin{bmatrix}
    v_1\\\\
    v_2\\\\
    v_3
\end{bmatrix}\times
\begin{bmatrix}
    w_1\\\\
    w_2\\\\
    w_3
\end{bmatrix} = det
\left(
\begin{bmatrix}
    \hat{i} & \hat{j} & \hat{k}\\\\
    v_1 & v_2 & v_3\\\\
    w_1 & w_2 & w_3
\end{bmatrix}
\right)
$$

关键点在于：为什么我们会在这个公式中将基向量作为一个矩阵元？实际上这里是一个符号上的技巧，当我们假设 $\hat{i},\hat{j},\hat{k}$均为数字时，写出来的行列式公式为：

$$
det
\left(
\begin{bmatrix}
    \hat{i} & v_1 & w_1\\\\
    \hat{j} & v_2 & w_2\\\\
    \hat{k} & v_3 & w_3
\end{bmatrix}
\right) = \hat{i}(v_2w_3-w_2v_3) + \hat{j}(v_3w_1-w_3v_1) + \hat{k}(v_1w_2-w_1v_2)
$$

此时行列式的结果恰好为三个基向量的线性组合，以一种直观的方式表示了输出向量的坐标
- 此时该向量是唯一一个与 $\vec{v}$ 和 $\vec{w}$ 垂直，长度是两个向量围成的平行四边形的面积，并且方向遵循右手定则的向量

### 叉积的对偶性（选读）

> 需要在完全了解了点积的对偶性的情况下，才能理解这部分的内容

对于上面那个奇怪的行列式，我们不考虑如何从数学角度上去推理为何这样的一个行列式恰好能够得到两个向量的叉积，而是从线性变换的角度上去思考：

和点积的对偶性相同，我们需要先假设存在这样一个线性变换，将三维空间的点映射到一条直线上（即秩为1），这个线性变换是由向量 $\vec{v}$ 和 $\vec{w}$ 定义的，然后理论上我们应当可以找到一个和该线性变换对应对偶向量，该向量恰好是 $\vec{v}$ 和 $\vec{w}$ 的叉积

首先不考虑上面的内容，我们已经知道了，对于一个二维空间，求两个向量围成的平行四边形的面积等价于求**以这两个向量为基向量的线性变换的行列式**，即：

$$
\begin{bmatrix}
    a\\\\
    b
\end{bmatrix}
\times
\begin{bmatrix}
    c\\\\
    d
\end{bmatrix} = det
\left(
    \begin{bmatrix}
        a & c\\\\
        b & d
    \end{bmatrix}
\right)
$$

要注意这里的**叉积**是一个不严谨的写法，它并不代表真正的叉积

将这个公式推广到三维空间，由三个向量围成的平行六面体的体积等价于**以这三个向量为基向量的线性变换的行列式**，即：

$$
\begin{bmatrix}
    u_1\\\\
    u_2\\\\
    u_3
\end{bmatrix}
\times
\begin{bmatrix}
    v_1\\\\
    v_2\\\\
    v_3
\end{bmatrix}
\times
\begin{bmatrix}
    w_1\\\\
    w_2\\\\
    w_3
\end{bmatrix} = det
\left(
    \begin{bmatrix}
        u_1 & v_1 & w_1\\\\
        u_2 & v_2 & w_2\\\\
        u_3 & v_3 & w_3
    \end{bmatrix}
\right)
$$

如果我们固定 $\vec{v}$ 和 $\vec{w}$，将这两个向量看作已知量，将 $\vec{u}$ 看作未知量的话，那么我们得到了一个关于平行六面体体积的函数：

$$
f\left(
    \begin{bmatrix}
        x\\\\
        y\\\\
        z
    \end{bmatrix}
\right) = det
\left(
    \begin{bmatrix}
        x & v_1 & w_1\\\\
        y & v_2 & w_2\\\\
        z & v_3 & w_3
    \end{bmatrix}
\right)
$$

不难证明，这个函数是一个线性的函数

在矩阵一章中我们已经知道了，矩阵实际上是对线性变换的描述，即一个**线性的函数**，因此我们可以将上述公式的 $f$ 替换为一个 $1\times 3$ 的矩阵：
- 关于为什么是$1\times 3$ 的矩阵：因为该矩阵接受一个三维向量作为输入，最终输出一个数（一维向量）代表平行六面体的体积

$$
\begin{bmatrix}
    ? & ? & ?
\end{bmatrix}
\begin{bmatrix}
    x\\\\
    y\\\\
    z
\end{bmatrix} = det
\left(
    \begin{bmatrix}
        x & v_1 & w_1\\\\
        y & v_2 & w_2\\\\
        z & v_3 & w_3
    \end{bmatrix}
\right)
$$

而考虑到点积的对偶性，我们必然能找到一个三维向量与该矩阵对应：
- 回顾一下点积对偶性的最后一段：当我们看到一个从高维到一维的线性变换时，必然存在一个向量 $\vec{u}$ 与该变换相关联，对向量 $\vec{v}$ 进行该线性变换等价于对 $\vec{v}$ 和 $\vec{u}$ 做点积

$$
\begin{bmatrix}
    p_1\\\\
    p_2\\\\
    p_3
\end{bmatrix} \cdot
\begin{bmatrix}
    x\\\\
    y\\\\
    z
\end{bmatrix} = det
\left(
    \begin{bmatrix}
        x & v_1 & w_1\\\\
        y & v_2 & w_2\\\\
        z & v_3 & w_3
    \end{bmatrix}
\right)
$$

展开等式左右两边，我们会得到下列式子：

$$
p_1\cdot x+p_2\cdot y+p_3\cdot z = x(v_2w_3-v_3w_2)+y(v_3w_1-v1w_3)+z(v_1w_2-v_2w_1)
$$

于是我们可以得到该**对偶向量**的坐标：

$$
\begin{cases}
    p1 = v_2w_3-v_3w_2\\\\
    p2 = v_3w_1-v1w_3\\\\
    p3 = v_1w_2-v_2w_1
\end{cases}
$$

不难发现，该对偶向量就是向量 $\vec{v}$ 和向量 $\vec{w}$ 做叉积得到的向量

回到上面的式子，等式右侧的行列式代表的是三个向量围成的平行六面体的体积

我们考虑等式左边的点积部分：

还记得在二维空间中，向量 $\vec{v}$ 和 $\vec{w}$ 的点积等于 $\vec{w}$ 在 $\vec{v}$ 上的投影长度，乘以 $\vec{v}$ 的长度。同样地，在左式中，该点积的含义是：未知向量在向量 $\vec{p}$ 上的投影长度，乘以向量 $\vec{p}$ 的长度
- 其中向量 $\vec{p}$ 的长度等于向量 $\vec{v}$ 和 $\vec{w}$ 围成的平行四边形的面积，方向垂直于平行四边形
- 而未知向量在向量 $\vec{p}$ 上的投影长度则是**和该平行四边形底面对应的高**

![](/images/posts/linear-crossproduct-2.png)

由此我们可以得出，向量 $\vec{v}$ 和 $\vec{w}$ 的叉积所得到的向量 $\vec{p}$ 有如下性质：将 $\vec{p}$ 与另一个输入向量 $\vec{u}$ 作点积，得到的数即为向量 $\vec{u},\vec{v},\vec{w}$ 三个向量围成的平行六面体的体积  

## 基变换

在矩阵一章中我们已经知道了，当发生线性变换时，基向量随之改变（也就意味着坐标系随之改变），对于任何一个原有的向量 $\vec{v}$，我们都可以通过应用新的基向量的线性组合来得到变换后的向量

这里我们是通过线性变换来引入新的坐标系，如果我们反过来——对于一个已有的，新的坐标系，如何去考虑同一个向量 $\vec{v}$ 在两个坐标系中的坐标表示呢

举例来说，对于我们原有的坐标系A，其中有

$$
\hat{i}=
\begin{bmatrix}
    1\\\\
    0
\end{bmatrix}
\hat{j}=
\begin{bmatrix}
    0\\\\
    1
\end{bmatrix}
$$

引入一个新的坐标系B，其中有

$$
\hat{i_B}=
\begin{bmatrix}
    2\\\\
    1
\end{bmatrix}
\hat{j_B}=
\begin{bmatrix}
    -1\\\\
    1
\end{bmatrix}
$$

现在，我们有一个基于坐标系B表示的向量，求它在坐标系A中的坐标

$$
\vec{v}=
\begin{bmatrix}
    -1\\\\
    2
\end{bmatrix}
$$

从线性组合的角度，这个问题很简单——我们已经知道了 $\vec{v}$ 是 $\hat{i_B}$ 和 $\hat{j_B}$ 的线性组合，也知道了$\hat{i_B}$ 和 $\hat{j_B}$ 在坐标系A中的坐标表示，那么我们可以求出 $\vec{v}$ 在坐标系A中的坐标：

$$
\vec{v}=
\begin{bmatrix}
    -1\\\\
    2
\end{bmatrix}=
-1\hat{i_b}+2\hat{j_b}=
\begin{bmatrix}
    -4\\\\
    1
\end{bmatrix}
$$

不难发现，这个过程等同于我们对坐标系A应用变换到坐标系B的线性变换，即可以写成：

$$
\vec{v}=
-1\hat{i_b}+2\hat{j_b}=
\begin{bmatrix}
    -1\\\\
    2
\end{bmatrix}
\begin{bmatrix}
    2 & 1\\\\
    -1 & 1
\end{bmatrix}=
\begin{bmatrix}
    -4\\\\
    1
\end{bmatrix}
$$

奇怪的一点是：如果我们翻译一下这个式子，即对于用坐标系B的基向量表示的向量坐标，对其应用**从坐标系A变换到坐标系B的线性变换**，可以得到用坐标系A的基向量表示的这个向量坐标
- 我们对一个向量应用了一次从坐标系A到坐标系B的线性变换
- 却将该向量从一个坐标系B的坐标表示转换为了一个坐标系A的坐标表示

关键点在于，和实际上的线性变换不同，这里的向量并没有发生变换，变换了的只有空间本身。

在先前的描述中，对于一次线性变换

$$
\begin{bmatrix}
    2 & 1\\\\
    -1 & 1
\end{bmatrix}
$$

我们说变换后的基向量坐标变为

$$
\hat{i_B}=
\begin{bmatrix}
    2\\\\
    1
\end{bmatrix}
\hat{j_B}=
\begin{bmatrix}
    -1\\\\
    1
\end{bmatrix}
$$

显然，这里新的基向量坐标，仍然是以旧的坐标系的基向量为基准来描述的

但是和先前的描述中有一点不同的是，这里的 $\vec{v}$ 是以新的基向量为基准描述的，要得到以旧的基向量为基准描述的 $\vec{v}$ ，我们需要先想出一个错误的，以以旧的基向量为基准描述的 $\vec{u}$，该向量的坐标和 $\vec{v}$ 完全相同

![](/images/posts/linear-basis-1.png)

对 $\vec{u}$ 应用该线性变换，即可得到以旧的基向量为基准描述的 $\vec{v}$

![](/images/posts/linear-basis-2.png)

反过来说，如果给定了一个以坐标系A的基向量表示的向量，求该向量在坐标系B的基向量表示，那么我们应当对该向量应用**从坐标系A到坐标系B的线性变换矩阵的逆矩阵**，或者说**从坐标系B到坐标系A的线性变换矩阵**

这些矩阵我们叫做**基变换矩阵**

### 基变换后的线性变换

我们先前的描述一直都是基于坐标系A的基向量描述的

给定一个线性变换：逆时针旋转90°，我们可以轻松地给出该变换的矩阵

$$
\begin{bmatrix}
    0 & -1\\\\
    1 & 0
\end{bmatrix}
$$

然而，在坐标系B视角中的同一个线性变换，这个矩阵是不适用的，因为矩阵本身是**对变换前后基向量的追踪**，因此在坐标系B的视角中，该变换的矩阵也应当描述**坐标系B视角下的基向量在变换后的坐标**，由于坐标系的不同，坐标系B的基向量在逆时针旋转90°后，并不对应该矩阵

换一个思路，对于一个坐标系B视角下给出的向量

$$
\vec{v}=
\begin{bmatrix}
    -1\\\\
    2
\end{bmatrix}
$$

我们要求出该向量在逆时针旋转90°后的坐标，可以如下操作：

首先，通过基变换矩阵，将该坐标转换为**坐标系A视角下描述的坐标**

$$
\begin{bmatrix}
    2 & 1\\\\
    -1 & 1
\end{bmatrix}
\begin{bmatrix}
    -1\\\\
    2
\end{bmatrix}
$$

然后，由于我们已知了坐标系A视角下该线性变换的矩阵，因此可以对其应用该线性变换

$$
\begin{bmatrix}
    0 & -1\\\\
    1 & 0
\end{bmatrix}
\begin{bmatrix}
    2 & 1\\\\
    -1 & 1
\end{bmatrix}
\begin{bmatrix}
    -1\\\\
    2
\end{bmatrix}
$$

但是，此时我们得到的是选择90°后的 $\vec{v}$ 在坐标系A下的坐标表示，因此我们对其应用基变换矩阵的逆矩阵，将其转换回坐标系B下的坐标表示

$$
\begin{bmatrix}
    2 & 1\\\\
    -1 & 1
\end{bmatrix}^{-1}
\begin{bmatrix}
    0 & -1\\\\
    1 & 0
\end{bmatrix}
\begin{bmatrix}
    2 & 1\\\\
    -1 & 1
\end{bmatrix}
\begin{bmatrix}
    -1\\\\
    2
\end{bmatrix}
$$

我们将这三个矩阵看做一个复合矩阵将其合并，可以得到

$$
\begin{bmatrix}
    \frac{1}{3} & \frac{-2}{3}\\\\
    \frac{5}{3} & \frac{-1}{3}
\end{bmatrix}
$$

因此，该矩阵就是坐标系B视角下，逆时针旋转90°这个线性变换的矩阵

总的来说，对于任何一个形如 $A^{-1}MA$的表达式，都暗示着数学上的某种转移，其中M代表我们所熟知的某个视角下的变换，而A则是视角上的转换——对于我们所熟知的视角和我们真正需要的视角之间

## 特征值和特征向量

对于许多线性变换，在线性变换前后，都会有一些向量保持在它原本的直线上——更专业地说，这些向量停留在了它们张成的空间上

例如，对于矩阵

$$
\begin{bmatrix}
    3 & 1\\\\
    0 & 2
\end{bmatrix}
$$

而言，基向量 $\hat{i}$ 就停留在了它张成的空间上，仅仅在长度上进行了缩放

![](/images/posts/linear-eigen-1.png)

实际上，对于该线性变换，整个x轴上的向量以及对角线上的向量都有该特殊性质——仅在长度上进行了缩放，而没有离开它张成的直线

![](/images/posts/linear-eigen-2.png)


具有这些特殊性质的向量我们称作**特征向量**，其中特征向量变换前后缩放的比例（上图中的2倍和3倍）叫做**特征值**，即衡量特征向量在变换中拉伸或压缩比例的因子

对于一个三维坐标系中的旋转变换，其特征向量是该旋转的**旋转轴**

![](/images/posts/linear-eigen-3.png)

显然地，描述该变换绕 $[2 ,3 ,1]^T$ 向量旋转30度比一个复杂的线性变换矩阵要直观得多，此外，对于旋转而言，其特征值必然为1

因此，当我们用一个矩阵描述线性变换时，确实能够详细地描述出变换，但是这对于理解该变换的效果起不到太多作用——因为线性变换本身并不依赖于具体的坐标系，理解线性变换的关键点在于较少地依赖特定的坐标系。
- 求出该矩阵的特征向量和特征值时有助于我们更好地理解一个线性变换的本质

### 特征向量的计算

考虑到特征向量的性质，我们可以列出如下公式：

$$
A\vec{v} = \lambda \vec{v}
$$

显然地，由于特征向量在线性变换前后只进行了缩放，因此必然存在一个值 $\lambda$ 满足**该值对向量做数乘与进行该线性变换等效**，这个值即为特征值

我们试图找到另一个线性变换，满足该线性变换等效于任何一个向量与 $\lambda$ 数乘，显然，该线性变换的矩阵为：

$$
\begin{bmatrix}
    \lambda & 0 & 0\\\\
    0 & \lambda & 0\\\\
    0 & 0 & \lambda
\end{bmatrix}=\lambda
\begin{bmatrix}
    1 & 0 & 0\\\\
    0 & 1 & 0\\\\
    0 & 0 & 1
\end{bmatrix}=
\lambda I
$$

因此我们也可以将上式写作：

$$
A\vec{v} = (\lambda I) \vec{v} \\\\
(A-\lambda I)\vec{v}=\vec{0} \\\\
其中 \vec{v} 是一个非零向量
$$

在行列式一章中，我们已经知道，线性变换降低了空间的维度时，才会导致某个非零向量在线性变换后变为零向量，此时**该线性变换的行列式为0**

因此我们实际上要找一个 $\lambda$，满足 $det(A-\lambda I)=0$

对于本章最开始的矩阵，我们可以列出这样一个式子

$$
det\left(
\begin{bmatrix}
    3-\lambda & 1\\\\
    0 & 2-lambda
\end{bmatrix}
\right)=(3-\lambda)(2-\lambda)-1·0=0
$$

显然地，对于该线性变换，特征值为2和3

将特征值代入 $(A-\lambda I)\vec{v}=\vec{0}$ 中，即可得到该特征值对应的特征向量集合

要注意的是，不是所有的线性变换都有特征向量和特征值，例如，在二维坐标系中逆时针旋转90°就没有特征向量，对于该变换，如果试图去求其特征值：

$$
det\left(
\begin{bmatrix}
    -\lambda & -1\\\\
    1 & -\lambda
\end{bmatrix}
\right)=(-\lambda)(-\lambda)-(-1·1)=\lambda ^2+1=0
$$

显然地，该二元一次方程无实数解
- 一般来说特征值中出现复数解往往都对应变换中的某种旋转，这里不展开细说

另一个要注意的是，特征值唯一并不代表特征向量分布在一条直线上，对于任何一个标量乘以单位矩阵（等效于数乘）得到的缩放矩阵都满足特征值等于该标量，但平面内的每一个向量都是特征向量

### 特征基

如果一个线性变换前后，基向量保持在它所在的那条直线上，举例来说：

$$
\begin{bmatrix}
    3 & 0\\\\
    0 & 2
\end{bmatrix}
$$

不难注意到，这个矩阵是一个**对角矩阵**，此时对角线上的每个值分别对应一个特征值，而所有的基向量都是特征向量

此时矩阵与自己多次相乘（即幂运算）满足这样的式子：

$$
\begin{bmatrix}
    3 & 0\\\\
    0 & 2
\end{bmatrix}^n 
\begin{bmatrix}
    x\\\\
    y
\end{bmatrix}=
\begin{bmatrix}
    3^n & 0\\\\
    0 & 2^n
\end{bmatrix}
\begin{bmatrix}
    x\\\\
    y
\end{bmatrix}=
\begin{bmatrix}
    3^n x\\\\
    2^n y
\end{bmatrix}
$$

反之，对一个**非对角矩阵进行幂运算则是一件非常痛苦的事情**

我们先切到另一个话题——对于一个线性变换，如果特征向量可以重新张成变换前的全空间，那么我们可以变换坐标系，使得**特征向量变成新的基向量**

举例来说，对于线性变换

$$
\begin{bmatrix}
    3 & 1\\\\
    0 & 2
\end{bmatrix}
$$

有特征向量

$$
\vec{a}=
\begin{bmatrix}
    1\\\\
    0
\end{bmatrix}
\vec{b}=
\begin{bmatrix}
    -1\\\\
    1
\end{bmatrix}
$$

回顾上一章（基变换）那么我们可以写出这样一个复合矩阵

$$
\begin{bmatrix}
    1 & -1\\\\
    0 & 1
\end{bmatrix}^{-1}
\begin{bmatrix}
    3 & 1\\\\
    0 & 2
\end{bmatrix}
\begin{bmatrix}
    1 & -1\\\\
    0 & 1
\end{bmatrix}=
\begin{bmatrix}
    3 & 0\\\\
    0 & 2
\end{bmatrix}
$$

该复合矩阵就是该线性变换**在变换后，以新的特征向量为基向量的坐标系下，对该线性变换的矩阵描述**

在计算这个复合矩阵后，我们会发现，这个复合矩阵**必然是一个对角矩阵**，而对角值即该变换的特征值
- 因为我们此时已经把特征向量作为了新的基向量，此时该线性变换在该坐标系中只会进行缩放
- 这个过程我们也叫做矩阵的**对角化**，该矩阵为**可对角化矩阵**

此时，这组基向量我们称为**特征基**

回到最开始的话题——如果我们要求一个**非对角矩阵的幂运算**，这为我们提供了一个思路，即先通过这种方式求该矩阵的**特征基**，在这个坐标系视角下计算该矩阵的n次幂，并在计算完成后再转换为标准坐标系
- 但是仍然要记住一点，只有满足特征向量可以重新张成变换前的全空间的线性变换可以这么做，换言之，我们需要足够多的特征向量才能将矩阵**对角化**

## 抽象向量空间

我们回到这个文章中最开始的问题上——如何定义向量
- 一方面，我们可以将向量理解为一个箭头，并用一组数字表示它的坐标
- 另一方面，我们也可以将向量理解为一组数字，并恰好用一个箭头来表示它

但是，不论这个向量是多少维的（4D也好，100D也好），一组数字总能用于表示一些真实的具体的概念，而高维的箭头却只是一个模糊的几何概念

我们在线性代数中描述的大部分概念——如线性变换、行列式、基变换等，实际上都是针对空间本身处理，而独立于坐标的；当我们用坐标去描述这些概念时，实际上往往都**取决于我们所选的基向量**。然而，行列式、特征向量等概念实际上都是**与坐标系无关的**，我们可以随便地改变所选的坐标系，而不会影响上述这些概念的核心本质
- 一个特征向量可能会因为我们换了一个坐标系而不得不用另一个坐标来描述它，但是这个特征向量本身从来都没有变化过

### 函数

对于**函数**而言，它虽然既不是箭头也不是一组数字，但仍然具有向量的特征
- 对于两个函数 $f(x)$ 和 $g(x)$，这两个函数的和函数 $(f+g)(x)$，在任意一点上的值等于这两个函数的值之和
  - 这一点和向量在相加时，**和向量**等于各向量的分量之和类似，只不过对于函数而言**有无数个值需要相加**
- 对于函数 $f(x)$ 与一个数相乘，结果函数在任意一点上的值等于这个函数的值乘以该数
  - 这一点也与向量的数乘类似

由于对于向量的操作本质上都可以被划分为**相加和数乘**两种，因此对于线性代数中的大部分概念（例如线性变换、零空间、点积等等）都可以应用到函数上

举例来说，函数的线性变换我们可以理解为**一个接收函数的变换，并返回另一个函数**，在函数这个层面上，这种变换我们叫做**算子**和**线性算子**
  - **求导**是一个典型的**函数的线性变换**即**线性算子**，因为它将一个函数变换到另一个函数
  - 线性代数中的**点积**对应为函数的**内积**、线性代数中的**特征向量**对应为函数的**特征函数**等等

这里涉及到了**线性**的严格定义，因为之前在描述线性变换时的定义在线性算子这里不再适用
- 可加性：对于一个变换 $L$，满足 $L(\vec{v}+\vec{w})=L(\vec{v})+L(\vec{w})$
- 成比例：对于一个变换 $L$，满足 $L(c\vec{v})=cL(\vec{v})$
- 这两个定义有时也被合并成**线性变换保持向量的加法运算和数乘运算**

这两个定义的一个重要推论就是**一个线性变换可以通过它对基向量的作用来完全描述**，从而使我们可以使用**矩阵向量乘法**

求导是一个线性运算：

- 把两个函数相加然后求导，等效于对两个函数求导然后分别相加

$$
\frac{d}{dx}(x^3+x^2)=\frac{d}{dx}(x^3)+\frac{d}{dx}(x^2)
$$

- 把一个函数和一个数相乘然后求导，等效于对该函数求导然后乘以这个数

$$
\frac{d}{dx}(4x^3)=4\frac{d}{dx}(x^3)
$$

因此理论上，我们可以用一个矩阵来描述求导
- 这一点比向量空间的线性变换要难一些，因为函数本身可以看作**一种无限维的向量**

我们来考虑对一个多项式的求导，首先我们需要定义**多项式空间**——该空间应当包含任意高次的多项式，不难观察到，多项式本身已经写成了线性组合的形式（形如 $1·x^2+3·x+5·1$），因此我们很自然低可以$x$的不同次幂作为**基函数**，即 $1,x,x^2,x^3,\dots,x^n$
- 由于多项式的次数可以无限高，因此基函数的集合也是无穷大的

那么对于多项式 $1·x^2+3·x+5·1$，我们可以写出它的向量坐标表示：

$$
\begin{bmatrix}
    5\\\\
    3\\\\
    1\\\\
    0\\\\
    0\\\\
    \dots
\end{bmatrix}
$$

由于任何一个多项式都只有有限项，所以它们的坐标都是形如**有限长的一串数，跟上无限个零**的形式

我们可以直接给出对多项式求导的线性算子矩阵：
- 和求线性变换的矩阵类似，我们对每一个基函数求导，并把结果放在对应列，就能得到求导的矩阵

$$
\begin{bmatrix}
    0 & 1 & 0 & 0 &\dots\\\\
    0 & 0 & 2 & 0 &\dots\\\\
    0 & 0 & 0 & 3 &\dots\\\\
    0 & 0 & 0 & 0 &\dots\\\\
    \vdots & \vdots & \vdots & \vdots & \ddots
\end{bmatrix}
$$

以 $1x^3+5x^2+4x+5$ 为例

$$
\begin{bmatrix}
    0 & 1 & 0 & 0 &\dots\\\\
    0 & 0 & 2 & 0 &\dots\\\\
    0 & 0 & 0 & 3 &\dots\\\\
    0 & 0 & 0 & 0 &\dots\\\\
    \vdots & \vdots & \vdots & \vdots & \ddots
\end{bmatrix}
\begin{bmatrix}
    5\\\\
    4\\\\
    5\\\\
    1\\\\
    \vdots
\end{bmatrix}=
\begin{bmatrix}
    1·4\\\\
    2·5\\\\
    3·1\\\\
    0\\\\
    \vdots
\end{bmatrix}
$$

于是结果多项式为 $3x^2+10x+4$ 

### 总结

回到这一章最开始的问题：什么是向量，现在你会发现向量不只局限于箭头和数组，还可以扩展到函数上，数学还有许多可以被视作向量的事物

实际上，只要你处理的对象具有数乘和相加的概念，都可以被抽象为向量——其中，由这些可以被视作向量的事物组成的集合，被叫做**向量空间**

其中，要让建立起来的理论和概念可以适用于向量空间，不仅要满足数乘和相加，还应当符合以下八条公理：
1. $\vec{u}+(\vec{v}+\vec{w})=(\vec{u}+\vec{v})+\vec{w}$
2. $\vec{v}+\vec{w}=\vec{w}+\vec{v}$
3. 存在零向量满足 $0+\vec{v}=\vec{v}$
4. 对于任何一个向量 $\vec{v}$ 都存在一个 $-\vec{v}$ 满足 $$vec{v}+($-\vec{v})=0$
5. $a(b\vec{v})=(ab)\vec{v}$
6. $1\vec{v}=\vec{v}$
7. $a(\vec{v}+\vec{w})=a\vec{v}+a\vec{w}$
8. $(a+b)\vec{v}=a\vec{v}+b\vec{v}$

从计算机的角度上说，这些公理像是一个**接口**，只要你要处理的事物实现了这些接口，就可以对其应用线性代数的各种结论
- 这样，线性代数的数学家（开发者？）就不需要考虑各种奇怪的向量空间定义了，他们只需要去用这些公理（接口）去推导出新的结论（服务）
- 而使用线性代数的人（调库的人？）只需要保证自己定义的空间很好的实现了这些公理（接口），就可以去使用线性代数下的各种结论（服务）了

这也是为什么教科书经常倾向于用**可加性和成比例**来定义线性变换，而非我们最开始提到的**平行且等距**，因为它们更想要给出一个不局限于数组/箭头的，更加具有普适性的定义

所以实际上数学家们实际上**并不关心向量本身是什么这个问题**，因为他们只负责了**定义公理和推导结论**，即计算机中的**定义接口和提供服务**，而具体实现接口并使用服务的人（也就是定义向量并使用结论的人），实际上是应用层面的人们