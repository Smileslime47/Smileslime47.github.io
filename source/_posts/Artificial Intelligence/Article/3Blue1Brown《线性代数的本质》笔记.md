---
title: 3Blue1Brown《线性代数的本质》笔记
author: Liu Yibang
date: 2024/08/24
categories: 
    - Artificial Intelligence
    - Article
mathjax: true
---

> [官方中文](https://www.bilibili.com/video/BV1ys411472E/)

> 尽管一批教授和教科书编者用关于矩阵的荒唐至极的计算内容
> 
> 掩盖了线性代数的简明性，但是鲜有与之相较更为初等的理论。
>
> —— 让·迪厄多内

国内的线性代数教材往往过分专注于其**数值计算**的部分而对其几何含义缺少解释，而线性代数有许多被教材所忽视的、可视化的直观理解，当真正理解了几何直观与数值计算的关系时，线性代数的细节和其在各种领域的应用就会显得合情合理

更重要的是，关于数值计算的部分，现代已经拥有了计算机来帮我们出来这部分问题，在实践中，我们更应当去关注概念层面的东西

基于相同的道理，这篇文章仅用于个人的学习笔记，在涉及到相关概念的理解上，个人强烈建议去观看作者的原视频

## 向量

三种看待向量的视角：
- 物理专业：空间中的箭头，由方向和长度所定义，一旦向量被定义就可以在空间中任意移动而不被改变
- 计算机专业：有序数列，此时向量只是一个“列表”的花哨说法
- 数学专业：概况以上两种观点，向量可以是任何东西——只要能保证向量相加和向量与数字相乘是有意义的即可（最抽象的概念）

**向量加法和向量数乘贯穿线性代数的始终**

和物理专业的看法有一定出入的是，线性代数中的向量往往**以坐标原点起始**
- 当向量以坐标原点起始时，就可以通过一个有序列表来表示向量的坐标，对应了计算机专业中的看法
- 以二维坐标系为例，向量的坐标由一对数构成，描述了如何从向量的起点（也就是原点）出发达到向量的终点，从而保证了**向量**和**坐标**是一一对应的关系
- 和点坐标不同的是，向量坐标往往竖着写，以和点坐标区分开，例如：

$$
\begin{bmatrix}
    -2 \\\\
    3
\end{bmatrix}
$$

### 向量相加

![](/images/posts/linear-vector-1.png)

以 $\vec{v}+\vec{w}$ 为例，即将 $\vec{w}$ 平移，使其起点对准 $\vec{v}$ 的终点，最终画一条从 $\vec{v}$ 起点指向 $\vec{w}$ 终点的向量
- 将向量看作空间上的移动，向量的加法则代表了移动的累加
- 先沿着 $\vec{v}$ 移动，再从终点沿着将 $\vec{w}$ 移动，在效果上等价于沿着 $\vec{v}+\vec{w}$ 移动

向量加法的原理可以同样延伸到数轴上——以 $2-5$ 为例，相当于从一位数轴的原点出发，向右移动 2，再向左移动 5，本质上与直接向左移动 -3 等效

在一维数轴的基础上延伸，二维向量的加法仍然是坐标的累加：

$$
\begin{bmatrix}
    1\\\\
    2
\end{bmatrix}+
\begin{bmatrix}
    3\\\\
    -1
\end{bmatrix}=
\begin{bmatrix}
    4\\\\
    1
\end{bmatrix}
$$

即先沿着 x 坐标轴移动 (1+3) 步，再沿着y坐标轴移动 (2-1) 步，从单个坐标轴的角度上看，这与前面提到的一维数轴的加法是一样的

### 向量数乘（缩放）

![](/images/posts/linear-vector-2.png)

向量数乘即将原向量延长\缩短为原本的n倍，当n为负数时，则说明运算结果的向量方向与原向量相反

$$
2\cdot
\begin{bmatrix}
    3\\\\
    1
\end{bmatrix}=
\begin{bmatrix}
    6\\\\
    2
\end{bmatrix}
$$

这类运算被称为向量的**缩放**（Scaling），而其中的$n$被称作**标量**（Scalar）
- 在线性代数中，单独数字唯一的用途基本上就是用于向量的缩放，因此通常来说，数字和标量这两个词可以互相替换

### 向量的线性组合

这里有另一种角度来看待向量的坐标

在二维坐标系中有两个特殊含义的向量
- x轴上的单位向量 $\hat{i}$，也被叫做**i帽**（i-hat）
- y轴上的单位向量 $\hat{j}$，也被叫做**j帽**（j-hat）
- 这些单位向量被叫做**坐标系的基向量**

于是，我们可以将向量的坐标看作**一组标量**，分别描述了该向量是**i帽和j帽分别经过何种缩放后，再进行相加得到的**

![](/images/posts/linear-vector-3.png)

因此，当我们用坐标描述一个向量，它同样依赖于我们**正在使用的基向量**，不同的基向量会导致不同的结果（例如，平面锐角坐标系）

我们可以发现，此时该向量是由两个基向量经过缩放和相加得到的。由此，我们可以引出：两个数乘向量的和被称作这两个向量的**线性组合**，即：

$$
a\vec{v} + b\vec{w}
$$

而在大部分情况下对于一对初始向量的线性组合，如果我们让两个标量都自由变化，我们可以得到一个平面中的任何一个向量
- 例外是当两个向量平行时，此时得到的向量方向被严格限制在了平行的直线上

其中，所有可以由  $a\vec{v} + b\vec{w}$ 得到的向量的集合，被称为**向量 $\vec{v}$ 和 $\vec{w}$ 张成的空间（span）**

因此，我们也可以说：
- 对于大部分二维向量对，它们张成的空间是所有二维向量的集合
- 对于共线的二维向量对，它们张成的空间就是一条直线上的向量的集合

我们扩展到三维空间，不考虑特殊情况，三维空间中的两个向量张成的空间仍然是这两个向量构成的平面，但当加上第三个向量时，我们往往能得到所有的三维向量，这三个向量所有可能的线性组合就会构成整个三维空间
- 同样地，例外是当第三个向量仍落在前两个向量的平面上时，此时得到的向量方向被严格限制在了平面上
- 当我们修改第三个向量的标量时，实际上是在推动前两个向量张成的平面沿着第三个向量的方向移动，从而扫过整个三维空间

在二维空间和三维空间中，都有一种例外情况：
- 我们在向量的线性组合 $a\vec{v} + b\vec{w}$ 中添加了一个向量 $\vec{u}$，但是并没有扩展这个线性组合张成的空间，这个时候这些向量被称作**线性相关的**（Linearly Dependent）
    - 在这种情况下，我们有 $\vec{u} = a\vec{v} + b\vec{w}$，其中a和b取某个值时该式子可以成立
- 反之，如果一组向量中每一组向量都为这个线性组合扩展了新的维度，那么我们就称这一组向量是**线性无关的**（Linearly Independent）
    - 在这种情况下，我们有 $\vec{u} \neq a\vec{v} + b\vec{w}$，其中a和b可以取所有值

基于以上概念，我们可以引出**空间的基**的严格定义：张成该空间的一个线性无关向量的集合

## 矩阵与线性变换

### 线性变换

线性变换（Linear Transformation）：接受一个向量并输出一个向量的变换
- 线性（Linear）指空间中的所有直线在变换后仍然是直线，且原点的位置没有发生改变
- 变换（Transformation）本质上是函数（function）的花哨说法，但与函数不同的是，**变换**一词在刻意地暗示你可以**用可视化的运动来思考这个过程**

![](/images/posts/linear-matrix-1.png)

这张图看似直线没有被弯曲，但是在加上对角线后就会发现对角线变得弯曲了，因此仍然不属于线性变换

![](/images/posts/linear-matrix-2.png)

总的来说，线性变换要保证**网格线平行且等距分布**，在这个大前提下，我们有一个重要的推论
- 变换前的向量 $\vec{v}$ 是**i帽与j帽**的线性组合 $a\hat{i_1} + b\hat{j_1}$，那么变换后的 $\vec{v}$ 仍是**变换后的i帽与j帽**的相同线性组合 $a\hat{i_2} + b\hat{j_2}$
- 换句话说，若在变换前有 $\vec{v} = -1 \hat{i} + 2 \hat{j}$，那么在变换后等式仍然成立，因此只要找到变换后的基向量代入式子，就能得到变换后的向量 $\vec{v}$

举例来说，对于向量

$$
\vec{v} = 
\begin{bmatrix}
    -1\\\\
    2
\end{bmatrix} = -1 \hat{i} + 2 \hat{j} = -1 
\begin{bmatrix}
    1\\\\
    0
\end{bmatrix} + 2 
\begin{bmatrix}
    0\\\\
    1
\end{bmatrix}
$$

在变换后，基向量坐标发生了改变，有

$$
\vec{v} = -1 \hat{i} + 2 \hat{j} = -1 
\begin{bmatrix}
    1\\\\
    -2
\end{bmatrix} + 2 
\begin{bmatrix}
    3\\\\
    0
\end{bmatrix} =
\begin{bmatrix}
    5\\\\
    2
\end{bmatrix}
$$

因此，只要我们知道了变换后的基向量，我们就能推出任意一个向量在变换后的位置。基于这点，我们可以说，一个二维线性变换仅有四个数字决定：变换后的i帽坐标和变换后的j帽坐标。

### 矩阵

于是，我们将这四个数字包装在一个**矩阵**（Matrix）中，用于定义一个线性变换的函数
- 也就是说，矩阵本质上是**对空间操纵的描述**
- 同样地，你也可以通过一个矩阵来想象线性变换的过程，只要分别移动i帽和j帽，然后另空间的其他部分随着基向量一起移动即可
- 例如：将坐标系逆时针旋转90度的矩阵就是：

$$
\begin{bmatrix}
    0 & -1\\\\
    1 & 0
\end{bmatrix}
$$

- 如果两个基向量是线性相关的，就意味着整个二维空间被挤压到它们所在的直线上

$$
\begin{bmatrix}
    2 & -2\\\\
    1 & -1
\end{bmatrix}
$$

同样以上例为例，将变换写成矩阵运算的形式：

$$
\vec{v} = 
\begin{bmatrix}
    1 & 3\\\\
    -2 & 0
\end{bmatrix}
\begin{bmatrix}
    -1\\\\
    2
\end{bmatrix} = -1
\begin{bmatrix}
    1\\\\
    -2
\end{bmatrix} + 2
\begin{bmatrix}
    3\\\\
    0
\end{bmatrix} =
\begin{bmatrix}
    5\\\\
    2
\end{bmatrix}
$$

在这种理解方式下，矩阵的**第一列**代表**变换后的第一个基向量**，**第二列**代表**变换后的第二个基向量**，更加通用的公式是：

$$
\vec{v} = 
\begin{bmatrix}
    a & b\\\\
    c & d
\end{bmatrix}
\begin{bmatrix}
    x\\\\
    y
\end{bmatrix} = x
\begin{bmatrix}
    a\\\\
    c
\end{bmatrix} + y
\begin{bmatrix}
    b\\\\
    d
\end{bmatrix} =
\begin{bmatrix}
    ax+by\\\\
    cx+dy
\end{bmatrix}
$$

## 矩阵乘法与线性变换复合

### 复合变换

我们已经知道了，矩阵是**对空间线性变换的描述函数**。和向量类似，我们也可以对多个线性变换进行累加操作（即先进行线性变换A，再在此基础上进行线性变换B），我们称之为**两个线性变换的复合变换**

举例来说，旋转（Rotation）变换的矩阵是：

$$
\begin{bmatrix}
    0 & -1\\\\
    1 & 0
\end{bmatrix}
$$

剪切（Shear）变换的矩阵是：

$$
\begin{bmatrix}
    1 & 1\\\\
    0 & 1
\end{bmatrix}
$$

通过对旋转后的基向量进行剪切变换，我们可以得到这两个变换的复合变换的矩阵，该矩阵描述了先进行旋转再进行变换后的总效应：

$$
\begin{bmatrix}
    1 & -1\\\\
    1 & 0
\end{bmatrix}
$$

如果我们想对一个向量 $\vec{v}$ 先应用旋转，再应用剪切的话，那么计算公式应当是：

$$
\begin{bmatrix}
    1 & 1\\\\
    0 & 1
\end{bmatrix}
\left(
    \begin{bmatrix}
        0 & -1\\\\
        1 & 0
    \end{bmatrix}
    \begin{bmatrix}
        x\\\\
        y
    \end{bmatrix}
\right) =
\begin{bmatrix}
    1 & -1\\\\
    1 & 0
\end{bmatrix}
\begin{bmatrix}
    x\\\\
    y
\end{bmatrix}
$$

值得一提的是，线性变换的公式是从右往左读的，按照这个公式，应当先应用右侧的旋转矩阵，再应用左侧的剪切矩阵。
- 这个习惯是基于函数写法的延续，举例来说，$f(x)$ 是对 $x$ 应用的函数，这里的 $f$ 也是写在左侧。如果我们把剪切矩阵记作函数 $g$，把旋转矩阵记作函数 $f$，把向量 $\vec{v}$ 记作 $x$ 的话，那么按照函数的写法就应当是 $g(f(x))$，可以看到这个顺序和上面的式子是相同的顺序

### 矩阵乘法

于是，我们称这个复合变换的矩阵为**旋转矩阵和剪切矩阵的积**
- 和向量不同的是，这里是积（乘法）而不是和（加法）

$$
\begin{bmatrix}
    1 & 1\\\\
    0 & 1
\end{bmatrix}
\begin{bmatrix}
    0 & -1\\\\
    1 & 0
\end{bmatrix} =
\begin{bmatrix}
    1 & -1\\\\
    1 & 0
\end{bmatrix}
$$

通过对线性变换过程中的基向量进行跟踪，我们可以很轻松地写出矩阵乘法的公式，对于：

$$
\begin{bmatrix}
    a & b\\\\
    c & d
\end{bmatrix}
\begin{bmatrix}
    e & f\\\\
    g & h
\end{bmatrix}
$$

复合函数的第一列列向量：

$$
\begin{bmatrix}
    a & b\\\\
    c & d
\end{bmatrix}
\begin{bmatrix}
    e\\\\
    g
\end{bmatrix} = e
\begin{bmatrix}
    a\\\\
    c
\end{bmatrix} + g
\begin{bmatrix}
    b\\\\
    d
\end{bmatrix} =
\begin{bmatrix}
    ae+bg\\\\
    ce+dg
\end{bmatrix}
$$

复合函数的第二列列向量：

$$
\begin{bmatrix}
    a & b\\\\
    c & d
\end{bmatrix}
\begin{bmatrix}
    f\\\\
    h
\end{bmatrix} = f
\begin{bmatrix}
    a\\\\
    c
\end{bmatrix} + h
\begin{bmatrix}
    b\\\\
    d
\end{bmatrix} =
\begin{bmatrix}
    af+bh\\\\
    cf+dh
\end{bmatrix}
$$

于是我们得出了复合变换的计算公式：

$$
\begin{bmatrix}
    a & b\\\\
    c & d
\end{bmatrix}
\begin{bmatrix}
    e & f\\\\
    g & h
\end{bmatrix} =
\begin{bmatrix}
    ae+bg & af+bh\\\\
    ce+dg & cf+dh
\end{bmatrix}
$$

- 矩阵乘法不满足交换律，即 $AB \neq BA$
- 矩阵乘法满足结合律，即 $(AB)C = A(BC)$
    - 如果你用公式推导的思路去做这个证明，会发现非常痛苦且没有帮助
    - 只从线性变换的角度上思考，两个公式本质上都描述了**先进行C变换，再进行B变换，最后进行A变换**，因此没有区别

## 行列式

在网格线平行且等距分布（即线性变换）的情况下，空间中任意一个区域的面积在变换后的缩放倍数都应当是一致的

举例来说，对于矩阵：

$$
\begin{bmatrix}
    3 & 2\\\\
    0 & 2
\end{bmatrix}
$$

对于空间中的任意一个区域 $A$，在经过该线性变换后体积变为原来的6倍，即 $6\cdot A$，那么我们称 $6$ 为该线性变换的**行列式**（determinant），即：

$$
det
\left(
    \begin{bmatrix}
        3 & 2\\\\
        0 & 2
    \end{bmatrix}
\right) = 6
$$

也就是说，**行列式**用于描述线性变换改变面积的比例
- 当一个线性变换的行列式为 0 时，说明该线性变换将空间压缩到了一个更小的维度上，即变换后的基向量是线性相关的：

$$
det
\left(
    \begin{bmatrix}
        4 & 2\\\\
        2 & 1
    \end{bmatrix}
\right)
=0
$$

- 当一个线性变换的行列式为负数时，说明该变换本质上将整个二维平面进行了一次翻转，我们也称这样的变换改变了空间的**定向**

推广到三维空间，行列式就是对体积变换比例的描述
- 当在三维空间考虑空间的定向时，可以用右手定则：右手食指是 $\hat{i}$ 的方向，中指是 $\hat{j}$ 的方向，此时大拇指应当自然地指向 $\hat{k}$ 的方向。当变换后如果仍能用右手这样指向，则行列式为正，如果只能用左手来指向，则行列式为负。

### 行列式的计算

先给出一个结论，对于一个二维的矩阵，行列式的计算公式是：

$$
det
\left(
    \begin{bmatrix}
        a & b\\\\
        c & d
    \end{bmatrix}
\right) = ad - bc
$$

如何理解这个公式？假设 b 和 c 均为0，那么这个矩阵实际上是对 $\hat{i}$ 和 $\hat{j}$ 的正向缩放，此时 $\hat{i}$ 沿 x 轴被缩放了 a 倍，而 $\hat{j}$ 沿 y 轴缩放了 d 倍，面积的变换倍率就是 ad 即

$$
det
\left(
    \begin{bmatrix}
        a & 0\\\\
        0 & d
    \end{bmatrix}
\right) = ad
$$

粗略来说，以 $\hat{i}$ 和 $\hat{j}$ 围成的平行四边形为例，即 a 和 d 实际上是代表了这个平行四边形，将**从原点出发的对角线**进行拉伸后的面积倍率；而 b 和 c 则是这个平行四边形，将**对角的对角线**进行拉伸后的面积倍率

从一个更详细的几何角度来说，行列式的推导过程如下：

$$
det
\left(
    \begin{bmatrix}
        a & b\\\\
        c & d
    \end{bmatrix}
\right) = (a+b)(c+d) - ac - bd - 2bc = ad - bc
$$

![Alt text](/images/posts/linear-determinant-1.png)

当一个由 $\hat{i}$ 和 $\hat{j}$ 围成的正方形（面积为1）被变换成如图所示的平行四边形时，可以结合上图看出平行四边形的面积计算公式

对于三维空间的行列式计算，遵循如下公式：

$$
det
\left(
    \begin{bmatrix}
        a & b & c\\\\
        d & e & f\\\\
        g & h & i
    \end{bmatrix}
\right) = a \cdot det
\left(
    \begin{bmatrix}
        e & f\\\\
        h & i
    \end{bmatrix}
\right) - b \cdot det
\left(
    \begin{bmatrix}
        d & f\\\\
        g & i
    \end{bmatrix}
\right) + c \cdot det
\left(
    \begin{bmatrix}
        d & e\\\\
        g & h
    \end{bmatrix}
\right)
$$

然而，三维以及更高维的行列式计算公式的推导过程难以描述，且对于线性代数的理解帮助不大，这里不再赘述
- 如果想要证明，可以自己试一下三维空间中平行六面体体积的推导过程
- 或者，去参考 Sal Khan 的推导过程

### 复合变换的行列式

对于两个矩阵相乘得到的复合矩阵，其行列式也等于两个矩阵的行列式相乘，即：

$$
det(M_1 M_2) = det(M_1)det(M_2)
$$

这个公式从数学角度上推导相当困难，但是类似矩阵乘法的结合律，我们同样可以从几何角度去思考这个问题：当进行了线性变换 $M_2$ 后，区域的面积变为了原来的 $det(M_2)$ 倍，再进行了线性变换 $M_1$ 后，区域的面积在此基础上又变为了刚才的 $det(M_1)$ 倍

## 逆矩阵、列空间与零空间

### 线性方程组

形如

$$
\begin{cases}
    2x+5y+3z=-3\\\\
    4x+0y+8z=0\\\\
    1x+3y+0z=2
\end{cases}
$$

的方程组叫做**线性方程组**

不难注意到，你可以将上述方程组写成**矩阵和向量乘法**的形式：

$$
\begin{bmatrix}
    2 & 5 & 3\\\\
    4 & 0 & 8\\\\
    1 & 3 & 0
\end{bmatrix}
\begin{bmatrix}
    x\\\\
    y\\\\
    z
\end{bmatrix} = 
\begin{bmatrix}
    -3\\\\
    0\\\\
    2
\end{bmatrix}
$$

将该式子展开后得到的方程组和上面的方程组完全相同

我们称该矩阵为系数矩阵 $A$，未知量向量为 $\vec{x}$，常数向量为 $\vec{v}$，于是我们可以将该方程组写作如下形式：

$$
A\vec{x}=\vec{v}
$$

于是，我们求解该方程组的几何意义变为了：给定一个线性变换 $A$ 和一个向量 $\vec{v}$，要找到一个向量 $\vec{x}$，使之经过该线性变换后与向量 $\vec{v}$ 重合

在 $A$ 的行列式不为 0 的情况下，我们可以找到一个唯一的向量 $\vec{x}$ 在经过变换后与 $\vec{v}$ 重合 —— 只需要对向量 $\vec{v}$ 进行该线性变换的逆向变换即可

### 逆矩阵

这个**线性变换 $A$ 的逆向变换**我们记作 $A$ 的逆矩阵 —— $A^{-1}$

在应用变换 $A$ 再应用变换 $A^{-1}$ 后会回到初始状态，也就是对于矩阵乘法 $A^{-1}A$ 会得到一个**什么也不做**的复合变换，我们称之为**恒等变换**

$$
A^{-1}A = 
\begin{bmatrix}
    1 & 0\\\\
    0 & 1
\end{bmatrix}
$$

要得到矩阵 $A$ 的逆矩阵，我们可以通过计算机来计算，这样我们就可以通过在上述方程的两边同时乘以 $A^{-1}$ 来得到答案：

$$
A\vec{x}=\vec{v}\\\\
A^{-1}A\vec{x}=A^{-1}\vec{v}\\\\
\vec{x}=A^{-1}\vec{v}
$$

但是当 $A$ 的行列式为0的情况下，以二维矩阵（二元方程组）为例，空间被压缩为一条直线，我们无法将一条直线**逆向**回一个平面，即**行列式为0的矩阵是不可逆的**，因此该方程组无法求解
- 无法求解并不代表无解，而是该方程组存在多个可行解，无法求出唯一解
- 当一个线性变换降维，对于一个输出向量 $\vec{v}$，必然存在多个可能的输入向量 $\vec{x}$；换句话说，当我们对 $\vec{v}$ 应用线性变换的逆矩阵时，必然有多个可能的输出 $\vec{x}$，而这一点破坏了函数**一个输入只能对应一个输出**的基本性质

### 秩和列空间

我们可以将某个线性变换后空间的维数称作**秩**（rank）
- 若一个线性变换在执行后空间变为了二维（不论空间之前是几维的），那么我们就称该线性变换的秩为2

对于一个线性变换，可能得到的所有输出向量 $A\vec{v}$ 的集合，我们称之为**线性变换 $A$ 的列空间**（Column Space）
- 你可以这么理解：矩阵的每一列代表了变换后的基向量，因此列空间就是**变换后的基向量所张成的空间**

因此，对于**秩**的更确切的定义是：某个线性变换的列空间的维数

线性变换无法扩展维数，因此秩不会超过矩阵的列数。
- 当矩阵的秩与列数相等时，我们称该矩阵为**满秩矩阵**（Full Rank）

对于满秩矩阵而言，由于线性变换必须保证原点位置不变，因此列空间中一定包含零向量，反过来说，零向量也是唯一变换后会落在原点的向量 —— 这也是为什么齐次线性方程组 $A\vec{x} = 0$ 一定有零解的原因，因为有且仅有零向量在经过任何变换后仍然是零向量

当矩阵的秩小于列数时，不仅有零向量在变换后会得到零向量，还有一系列其他方向上的向量被压缩到原点，此时对于齐次线性方程组 $A\vec{x} = 0$，有一系列可能的向量解，我们称之为该矩阵的**零空间**或**核**

## 点积和叉积

### 点积

对于两个相同维度向量的点积，只需要将对应的坐标两两相乘，最后求和即可，即：

$$
\begin{bmatrix}
    a\\\\
    b\\\\
    c
\end{bmatrix}
\cdot
\begin{bmatrix}
    d\\\\
    e\\\\
    f
\end{bmatrix} = ad + bc + cf
$$

对于这个式子的几何含义：以 $\vec{v} \cdot \vec{w}$ 为例，即 $\vec{w}$ 在 $\vec{v}$ 上的投影长度，乘以向量 $\vec{v}$ 的长度
- 当然，点积是满足交换律的，因此该几何含义中将 $\vec{v}$ 和 $\vec{w}$ 的顺序调换也是可以的
- 其中点积为负数表示即 $\vec{w}$ 在 $\vec{v}$ 上的投影与 $\vec{v}$ 的方向相反

![](/images/posts/linear-dotproduct-1.png)

但是为什么点积的几何含义会和向量的投影联系起来？这里需要引入一个叫做**对偶性**的概念

### 对偶性

> 这部分比较难以理解，可能要对原视频多看几遍
> 
> 不过我个人认为，视频中对于将 $\hat{u}$ 推广到非单位向量的情况解释得并不够好，可以结合我的笔记来更好地理解这部分内容

对偶性指**两种数学事物中自然而又出乎意料的对应关系**，接下来我们要证明**一个多维空间到一个一维空间的线性变换的对偶是多维空间中的某个特定向量**

对于会降维的线性变换，要求原本等距分布的点在变换后仍然等距分布的，在这个基础上，我们去考虑如何描述一个降维的线性变换：

我们已经知道了，矩阵描述的是线性变换之后，基向量的坐标。在变换之前，我们在二维空间中拥有两个基向量：$\hat{i}$ 和 $\hat{j}$，但是由于变换之后空间降到了一维，因此变换后的基向量坐标也是一维的，即每个基向量各落在了一维数轴的一个数上，此时该变换的矩阵可能写作下列形式：

$$
\begin{bmatrix}
    1 & -2
\end{bmatrix}
$$

即一个非方矩阵，此时若我们想对一个向量 $\vec{v}$ 应用该线性变换，那么应该写作下列形式：

$$
\begin{bmatrix}
    1 & -2
\end{bmatrix}
\begin{bmatrix}
    4\\\\
    3
\end{bmatrix} = 4
\begin{bmatrix}
    1 
\end{bmatrix} + 3
\begin{bmatrix}
    -2
\end{bmatrix} = 
\begin{bmatrix}
    -2
\end{bmatrix} 
$$

不难注意到，若将该矩阵看作一个**颠倒的向量**，则此时计算方式等同于点积的计算方式

假设存在这样一个变换，将二维空间中的点投影到一条过原点的直线上，我们定义该直线上的基向量为 $\hat{u}$。显然地，这是一个接受二维向量并输出一维向量（数）的降维变换

![](/images/posts/linear-dotproduct-2.png)

要注意的是，图中的点表示二维向量的终点坐标，而不是点，你可以认为每个点都是一个**从原点出发指向该点的向量**

理所当然的，这个变换的矩阵应该形如

$$
\begin{bmatrix}
    a & b
\end{bmatrix}
$$

其中 $a$ 和 $b$ 分别代表了 $\hat{i}$ 和 $\hat{j}$ 投影到该直线上的坐标

![](/images/posts/linear-dotproduct-3.png)

首先考虑 $a$，即 $\hat{i}$ 的部分，由于 $\hat{i}$ 和 $\hat{u}$均为单位向量，他们之间存在一条对称轴将其分开，使之完全对称 —— 这导致了 $\hat{i}$ 在 $\hat{u}$ 上的投影长度等于  $\hat{u}$ 在 $\hat{i}$ 上的投影长度，而后者恰好是 $\hat{u}$ 的 x 轴坐标，即 $u_x$

![](/images/posts/linear-dotproduct-4.png)

将其推广到 $b$，即 $\hat{j}$ 的部分，不难得出 $\hat{j}$ 在 $\hat{u}$ 上的投影长度为 $u_y$

因此该矩阵为：

$$
\begin{bmatrix}
    u_x & u_y
\end{bmatrix}
$$

此时，我们随即给出一个二维空间的向量 $\vec{v}$ ，要求该向量在该直线上的投影，即：

$$
\begin{bmatrix}
    u_x & u_y
\end{bmatrix}
\begin{bmatrix}
    a\\\\
    b
\end{bmatrix} = 
\begin{bmatrix}
    a \cdot u_x + b \cdot u_y
\end{bmatrix} 
$$

要注意的是，以上情况为 $\hat{u}$ 的长度等同于变换前基向量长度（也就是1）的情况，只有最开始 $\hat{u}$ 定义为单位向量的时候， $\hat{u}$ 和 $\hat{i}$、$\hat{j}$ 的对称性才是成立的，此时该线性变换的含义是：求向量到该直线的投影长度

当 $\hat{u}$ 的长度为 $n$ 时，该对称性被破坏，实际上 $\hat{u}$ 在 x 轴上的投影长度是 $\hat{i}$ 在 $\hat{u}$ 上投影长度的 n 倍，要满足对称性，**该线性变换的定义必须发生改变**，我们必须将结果（原先是 $\hat{i}$ 在 $\hat{u}$ 上投影长度）也变为原先的 n 倍，即从原先的**求向量到直线的投影长度**变为**求向量到直线的投影长度和 n 的积**

此时这个线性变换的性质发生了改变，它不再代表**二维空间上的向量在该直线上的投影长度**，而是**二维空间上的向量在该直线上的投影长度乘以 $\hat{u}$ 的长度**，也就是说，对于公式

$$
A\vec{v} = 
\begin{bmatrix}
    u_x & u_y
\end{bmatrix}
\begin{bmatrix}
    a\\\\
    b
\end{bmatrix} = 
\begin{bmatrix}
    a \cdot u_x + b \cdot u_y
\end{bmatrix} 
$$

它代表的含义是向量 $\vec{v}$ 在向量 $\vec{u}$ 上的投影长度，乘以 $\hat{u}$ 的长度，这点**与最开始点积的定义符合**

这一点给我们的启发是：当我们看到一个从二维到一维的线性变换时，必然存在一个向量 $\vec{u}$ 与该变换相关联，对向量 $\vec{v}$ 进行该线性变换等价于对 $\vec{v}$ 和 $\vec{u}$ 做点积 
